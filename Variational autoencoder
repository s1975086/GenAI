import torch
import torchvision
from torchvision import datasets
from torchvision.transforms import ToTensor
import torchvision.transforms as transforms
from torch import nn
from torch.utils.data import DataLoader
import pandas as pd
import torch.optim as optim
import torch.utils.data as datas
import matplotlib.pyplot as plt
import torch.nn.functional as F

# For repeatability
torch.manual_seed(123)
torch.autograd.set_detect_anomaly(True)
# Saves weights and biases for both the enconder and decoder according to the number of channels
def save():
    torch.save(e.state_dict(),f'e_{no_channels1}.pt' )
    torch.save(d.state_dict(),f'd_{no_channels1}.pt' )
# Loads the saved values in a file according to the channels
def load():
    e.load_state_dict(torch.load(f'e_{no_channels1}.pt'))
    d.load_state_dict(torch.load(f'd_{no_channels1}.pt'))
    e.eval()
    d.eval()
# Taking an excel file's information about kernel sizes, strides and padding and turning it into a pandas table
kernel_excel=pd.read_excel(r'C:\Users\taken_out\for_privacy_reasons.xlsx',index_col=0)
# Returns the kernel, stride and padding values of each layer from the pandas table previously created
def layer_values(layer):  
    a=kernel_excel.iloc[layer-1]
    return int(a['kernel']),int(a['stride']),int(a['padding'])
# Returns the size at the end of each layer from the pandas table previously created
def final_size(layer):
    if kernel_excel.iloc[layer-1][4]%1==0:
        return int(kernel_excel.iloc[layer-1][4])
    else:
        print('Final size is decimal. Check the Excel file')
# Path where data is stored
dataroot= r'C:\Users\taken_out\for_privacy_reasons'
# Image size before data is processed
image_size=28
# Number of samples per batch
BATCH_SIZE = 256
# Loading data. MNIST for numbers, FashionMNIST for clothes and resizing it, making it grayscale and turning into a tensor
data=datasets.MNIST(
    root="data",
    train=True,
    download=True, 
    transform=transforms.Compose([transforms.Resize(image_size),
                               transforms.CenterCrop(image_size),
                               transforms.Grayscale(),
                               transforms.ToTensor(),
                               transforms.Normalize(mean=0,std=1,inplace=False),
                           ]))
# Amount of data overall
amount=16384
data, trash = datas.random_split(data,[amount,len(data)-amount])
# Creating batches
dataloader = DataLoader(data,
    batch_size=BATCH_SIZE,
    shuffle=True
)
# Initialising weights with a mean of 0 and a standard deviation of 0.02
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# 1 for grayscale and 3 for coloured pictures
no_colours=1
# Number of convolution layers
no_layers=2
# Final size after all convolution layers. In this case, 2.
size=final_size(2)**2
# Gradual decrease of the size after the convolution layers
s1=int(size/1.6)
s2=int(size/2.2)
s3=int(size/2.8)
s4=int(size/3.4)
s5=int(size/4)
# Number of channels in-between convolution layers
no_channels1=3
# Referring to first and second layer's k=kernel side size, s=stride, p=padding
l1k,l1s,l1p=layer_values(1)
l2k,l2s,l2p=layer_values(2)

# Class to encode the data
class Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        # Neural network
        self.main=nn.Sequential(
        nn.Conv2d( no_colours, no_channels1 , kernel_size=l1k, stride=l1s, padding=l1p, bias=False),
        nn.BatchNorm2d(no_channels1),
        nn.LeakyReLU(0.2),
        nn.Conv2d( no_channels1, 1 , kernel_size=l2k, stride=l2s, padding=l2p, bias=False),
        nn.BatchNorm2d(1))
        self.main2=nn.Sequential(
        nn.LeakyReLU(0.2),
        nn.Linear(size,s1),
        nn.LeakyReLU(0.2),
        nn.Linear(s1,s2),
        nn.Sigmoid(),
        nn.Linear(s2,s3),
        nn.LeakyReLU(0.2),
        nn.Linear(s3,s4),
        nn.LeakyReLU(0.2),
        nn.Linear(s4,s5),
        nn.LeakyReLU(0.2)
        )
        self.mu=nn.Linear(s5,s5)
        self.var=nn.Linear(s5,s5)
        self.sigmoid=nn.Sigmoid()
        self.logsoftmax=nn.LogSoftmax(dim=1)
        # Normal distribution
        self.distribution=torch.normal(0,1,size=(1,s5))
    def forward(self,input):
        input2=self.main(input).squeeze().flatten(1,2)
        return self.logsoftmax(self.mu(self.main2(input2))+self.var(self.main2(input2))*self.distribution)
    
# Creating an encoder
e=Encoder()
# Initializing the weights on that encoder
e.apply(weights_init)
# Gradually increasing intengers to the original number
s1_2=int(size2/3.4)
s2_2=int(size2/2.8)
s3_2=int(size2/2.2)
s4_2=int(size2/1.6)
# Original number of pixels in the picture
size2=image_size**2

# Class to decode data
class Decoder(nn.Module):
    def __init__ (self):
        super().__init__()
        # Neural network
        self.main=nn.Sequential(
        nn.LeakyReLU(0.2),
        nn.Linear(s5,s1_2),
        nn.LeakyReLU(0.2),
        nn.Linear(s1_2,s2_2),
        nn.LeakyReLU(0.2),
        nn.Linear(s2_2,s3_2),
        nn.LeakyReLU(0.2),
        nn.Linear(s3_2,s4_2),
        nn.LeakyReLU(0.2),
        nn.Linear(s4_2,size2),
        nn.Sigmoid()
        )
    def forward(self,input):
        return self.main(input).unflatten(1,[image_size,image_size])

# Creating a decoder
d=Decoder()
# Initializing the weights on that decoder
d.apply(weights_init)
# Learning rate
lr=0.00001 
# Times processing all batches of data
epochs=100
# Kullback-Leibler Divergence Loss Function to make sure the distribution tends to normal distribution
criterion=nn.KLDivLoss(reduction='batchmean',log_target=True)
# Cross-Entropy Loss Function for classification problems. Also can try nn.NLLLoss (Negative Log-Likelihood Loss Function)
criterion2=nn.BCELoss(reduction='sum')
optimizer_e = optim.Adam(e.parameters(), lr=lr, betas=(0.5, 0.999))
optimizer_d = optim.Adam(d.parameters(), lr=lr, betas=(0.5, 0.999))
# Iterations
iters=0
error_list=[]
iters_list=[]
# Trying to load saved weigths for the specific number of channels in use
try:
    load()
    print('Loaded file')
except:
    print('''No File
          New File for this number of channels must be created''')

print('Initializing...')
for epoch in range(epochs):
    print(f'Starting [{epoch+1}]/[{epochs}]')
    for tensor, label in dataloader:
        # Setting gradients to 0
        optimizer_e.zero_grad()  
        optimizer_d.zero_grad()  
        # Calculating the output after going through the encoder and the decoder
        output=d(e(tensor))
        # Calculating the error between before and after the data is processed
        error2=criterion2(output,tensor.squeeze())
        # Calculating how different the distributions are to normal for regularization purposes
        error3=criterion(e(tensor),F.log_softmax(torch.normal(0,1,size=(1,s5)),dim=1))
        # Summing the errors
        error=error3+error2
        # Calculating the gradient
        error.backward()
        # Updating the optimizers
        optimizer_e.step()
        optimizer_d.step()
        # Keeping track of the errors
        if iters%50==0:
            iters_list.append(iters)
            error_list.append(error.item())
            print(f'Current error = {error}')
        iters+=1

print('Finished')


######################################################### Extras ########################################################################
# Plots the original data, the data after going through the encoder and deocder and the output of random noise going through the system
fig=plt.figure(figsize=(10,10))
noise=torch.rand(15,1,image_size,image_size)
print(noise.shape)
noise_result=d(e(noise))[10].detach()
for i,j in dataloader:
    n=4
    fig.add_subplot(2, 2, 1)
    plt.imshow(d(e(i))[n].squeeze().detach(),cmap='gray')
    plt.axis(False)
    plt.title('Processed')
    fig.add_subplot(2, 2, 2)
    plt.imshow(i[n].squeeze(),cmap='gray')
    plt.axis(False)
    plt.title('Original')
    fig.add_subplot(2, 2, 3)
    plt.imshow(noise_result.squeeze(),cmap='gray')
    plt.axis(False)
    plt.title('Generated')
    break
##################################################
# Labelling and plotting data points in latent space
from sklearn.manifold import TSNE
import plotly.express as px

label_dict={0:'0',
1: '1',
2: '2',
3: '3',
4: '4',
5: '5',
6: '6',
7: '7',
8: '8',
9: '9'}

encoded_samples=[]
encoded_samples_0 = []

labels_group=[]
labels_group_0=[]

for image, labels in dataloader: 

    img = image
    img2=e(img).detach()
    encoded_samples_0.append(img2)
    labels_group_0.append(labels)


for i in encoded_samples_0:
    for j in i:
        encoded_samples.append(j)

for i in labels_group_0:
    for j in i:
        labels_group.append(label_dict[j.item()])


final_list=[]
it=0
for i in encoded_samples:
    sample={f"Enc. Variable {j}": enc for j, enc in enumerate(i)}
    sample['label'] = labels_group[it]
    final_list.append(sample)
    it+=1
final_list=pd.DataFrame(final_list)

tsne = TSNE() 
tsne_results = tsne.fit_transform(final_list.drop(['label'],axis=1)) 
fig = px.scatter(tsne_results, x=0, y=1, color=final_list.label.astype(str),labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'}) 
fig.show()
